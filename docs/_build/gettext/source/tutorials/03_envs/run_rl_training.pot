# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022-2024, The Isaac Lab Project Developers.
# This file is distributed under the same license as the Isaac Lab package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Isaac Lab 1.0.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-07-04 11:04+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../source/tutorials/03_envs/run_rl_training.rst:2
msgid "Training with an RL Agent"
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:6
msgid "In the previous tutorials, we covered how to define an RL task environment, register it into the ``gym`` registry, and interact with it using a random agent. We now move on to the next step: training an RL agent to solve the task."
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:10
msgid "Although the :class:`envs.ManagerBasedRLEnv` conforms to the :class:`gymnasium.Env` interface, it is not exactly a ``gym`` environment. The input and outputs of the environment are not numpy arrays, but rather based on torch tensors with the first dimension being the number of environment instances."
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:15
msgid "Additionally, most RL libraries expect their own variation of an environment interface. For example, `Stable-Baselines3`_ expects the environment to conform to its `VecEnv API`_ which expects a list of numpy arrays instead of a single tensor. Similarly, `RSL-RL`_, `RL-Games`_ and `SKRL`_ expect a different interface. Since there is no one-size-fits-all solution, we do not base the :class:`envs.ManagerBasedRLEnv` on any particular learning library. Instead, we implement wrappers to convert the environment into the expected interface. These are specified in the :mod:`omni.isaac.lab_tasks.utils.wrappers` module."
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:23
msgid "In this tutorial, we will use `Stable-Baselines3`_ to train an RL agent to solve the cartpole balancing task."
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:28
msgid "Wrapping the environment with the respective learning framework's wrapper should happen in the end, i.e. after all other wrappers have been applied. This is because the learning framework's wrapper modifies the interpretation of environment's APIs which may no longer be compatible with :class:`gymnasium.Env`."
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:33
msgid "The Code"
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:35
msgid "For this tutorial, we use the training script from `Stable-Baselines3`_ workflow in the ``source/standalone/workflows/sb3`` directory."
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:0
msgid "Code for train.py"
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:47
msgid "The Code Explained"
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:51
msgid "Most of the code above is boilerplate code to create logging directories, saving the parsed configurations, and setting up different Stable-Baselines3 components. For this tutorial, the important part is creating the environment and wrapping it with the Stable-Baselines3 wrapper."
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:55
msgid "There are three wrappers used in the code above:"
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:57
msgid ":class:`gymnasium.wrappers.RecordVideo`: This wrapper records a video of the environment and saves it to the specified directory. This is useful for visualizing the agent's behavior during training."
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:60
msgid ":class:`wrappers.sb3.Sb3VecEnvWrapper`: This wrapper converts the environment into a Stable-Baselines3 compatible environment."
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:62
msgid "`stable_baselines3.common.vec_env.VecNormalize`_: This wrapper normalizes the environment's observations and rewards."
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:65
msgid "Each of these wrappers wrap around the previous wrapper by following ``env = wrapper(env, *args, **kwargs)`` repeatedly. The final environment is then used to train the agent. For more information on how these wrappers work, please refer to the :ref:`how-to-env-wrappers` documentation."
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:70
msgid "The Code Execution"
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:72
msgid "We train a PPO agent from Stable-Baselines3 to solve the cartpole balancing task."
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:75
msgid "Training the agent"
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:77
msgid "There are three main ways to train the agent. Each of them has their own advantages and disadvantages. It is up to you to decide which one you prefer based on your use case."
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:81
msgid "Headless execution"
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:83
msgid "If the ``--headless`` flag is set, the simulation is not rendered during training. This is useful when training on a remote server or when you do not want to see the simulation. Typically, it speeds up the training process since only physics simulation step is performed."
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:93
msgid "Headless execution with off-screen render"
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:95
msgid "Since the above command does not render the simulation, it is not possible to visualize the agent's behavior during training. To visualize the agent's behavior, we pass the ``--enable_cameras`` which enables off-screen rendering. Additionally, we pass the flag ``--video`` which records a video of the agent's behavior during training."
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:104
msgid "The videos are saved to the ``logs/sb3/Isaac-Cartpole-v0/<run-dir>/videos`` directory. You can open these videos using any video player."
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:108
msgid "Interactive execution"
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:112
msgid "While the above two methods are useful for training the agent, they don't allow you to interact with the simulation to see what is happening. In this case, you can ignore the ``--headless`` flag and run the training script as follows:"
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:120
msgid "This will open the Isaac Sim window and you can see the agent training in the environment. However, this will slow down the training process since the simulation is rendered on the screen. As a workaround, you can switch between different render modes in the ``\"Isaac Lab\"`` window that is docked on the bottom-right corner of the screen. To learn more about these render modes, please check the :class:`sim.SimulationContext.RenderMode` class."
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:127
msgid "Viewing the logs"
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:129
msgid "On a separate terminal, you can monitor the training progress by executing the following command:"
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:137
msgid "Playing the trained agent"
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:139
msgid "Once the training is complete, you can visualize the trained agent by executing the following command:"
msgstr ""

#: ../../source/tutorials/03_envs/run_rl_training.rst:146
msgid "The above command will load the latest checkpoint from the ``logs/sb3/Isaac-Cartpole-v0`` directory. You can also specify a specific checkpoint by passing the ``--checkpoint`` flag."
msgstr ""
