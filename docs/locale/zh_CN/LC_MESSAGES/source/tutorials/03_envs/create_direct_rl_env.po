# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022-2024, The Isaac Lab Project Developers.
# This file is distributed under the same license as the Isaac Lab package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Isaac Lab 1.0.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-07-04 11:04+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: Ziqi Fan <fanziqi614@gmail.com>\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"Generated-By: Babel 2.15.0\n"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:5
msgid "Creating a Direct Workflow RL Environment"
msgstr "创建直接工作流RL环境"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:9
msgid ""
"In addition to the :class:`envs.ManagerBasedRLEnv` class, which encourages "
"the use of configuration classes for more modular environments, the "
":class:`~omni.isaac.lab.envs.DirectRLEnv` class allows for more direct "
"control in the scripting of environment."
msgstr ""
"除了 :class:`envs.ManagerBasedRLEnv` "
"class外，该class鼓励使用更模块化环境的配置classes，:class:`~omni.isaac.lab.envs.DirectRLEnv` "
"class允许在脚本化环境的情况下更直接地进行控制。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:13
msgid ""
"Instead of using Manager classes for defining rewards and observations, the "
"direct workflow tasks implement the full reward and observation functions "
"directly in the task script. This allows for more control in the "
"implementation of the methods, such as using pytorch jit features, and "
"provides a less abstracted framework that makes it easier to find the "
"various pieces of code."
msgstr ""
"直接工作流任务不使用Manager "
"classes来定义奖励和观察，直接在任务脚本中直接实现完整的奖励和观察功能。这允许在方法的实现中更多地控制，比如使用pytorch "
"jit功能，并提供了一个更不抽象的框架，使得更容易找到各种代码片段。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:19
msgid ""
"In this tutorial, we will configure the cartpole environment using the "
"direct workflow implementation to create a task for balancing the pole "
"upright. We will learn how to specify the task using by implementing "
"functions for scene creation, actions, resets, rewards and observations."
msgstr ""
"在这个教程中，我们将使用直接工作流实现来配置cartpole环境，以创建一个平衡竖着的杆子的任务。我们将学习如何通过实现场景创建函数，actions，resets，rewards和observations来指定任务。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:25
msgid "The Code"
msgstr "代码"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:27
msgid ""
"For this tutorial, we use the cartpole environment defined in "
"``omni.isaac.lab_tasks.direct.cartpole`` module."
msgstr "在本教程中，我们使用 ``omni.isaac.lab_tasks.direct.cartpole`` 模块中定义的cartpole环境。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst
msgid "Code for cartpole_env.py"
msgstr "cartpole_env.py的代码"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:38
msgid "The Code Explained"
msgstr "代码解释"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:40
msgid ""
"Similar to the manager-based environments, a configuration class is defined "
"for the task to hold settings for the simulation parameters, the scene, the "
"actors, and the task. With the direct workflow implementation, the "
":class:`envs.DirectRLEnvCfg` class is used as the base class for "
"configurations. Since the direct workflow implementation does not use Action"
" and Observation managers, the task config should define the number of "
"actions and observations for the environment."
msgstr ""
"与基于manager的环境类似，为了保存设置参数，场景，actors和任务的配置class需要被定义。使用直接工作流实现时，基类的配置是 :class:`envs.DirectRLEnvCfg`"
" class。由于直接工作流实现不使用动作和观察manager，任务配置应该定义环境中的actions和observations的数量。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:55
msgid ""
"The config class can also be used to define task-specific attributes, such "
"as scaling for reward terms and thresholds for reset conditions."
msgstr "配置类也可以用来定义具体任务的属性，比如奖励项的缩放和重置条件的阈值。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:74
msgid ""
"When creating a new environment, the code should define a new class that "
"inherits from :class:`~omni.isaac.lab.envs.DirectRLEnv`."
msgstr ""
"当创建一个新的环境时，代码应该定义一个新的class，该class继承自 "
":class:`~omni.isaac.lab.envs.DirectRLEnv` 。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:84
msgid ""
"The class can also hold class variables that are accessible by all functions"
" in the class, including functions for applying actions, computing resets, "
"rewards, and observations."
msgstr ""
"这个class还可以定义可以被该class中的所有函数访问的class变量，包括应用actions、计算resets、rewards和observations的函数。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:88
msgid "Scene Creation"
msgstr "场景创建"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:90
msgid ""
"In contrast to manager-based environments where the scene creation is taken "
"care of by the framework, the direct workflow implementation provides "
"flexibility for users to implement their own scene creation function. This "
"includes adding actors into the stage, cloning the environments, filtering "
"collisions between the environments, adding the actors into the scene, and "
"adding any additional props to the scene, such as ground plane and lights. "
"These operations should be implemented in the ``_setup_scene(self)`` method."
msgstr ""
"与基于manager的环境相反，在这里场景的创建被框架处理，直接工作流实现提供了灵活性，让用户可以自己实现他们的场景创建函数。这包括在主舞台中添加actors，克隆环境，过滤环境间的碰撞，将actors添加到场景中，并在场景中添加任何其他的道具，比如地面平面和灯光。这些操作应该在"
" ``_setup_scene(self)`` 方法中实现。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:102
msgid "Defining Rewards"
msgstr "定义奖励"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:104
msgid ""
"Reward function should be defined in the ``_get_rewards(self)`` API, which "
"returns the reward buffer as a return value. Within this function, the task "
"is free to implement the logic of the reward function. In this example, we "
"implement a Pytorch JIT function that computes the various components of the"
" reward function."
msgstr ""
"奖励函数应该在 ``_get_rewards(self)`` "
"API中定义，它将奖励缓冲区作为返回值。在这个函数中，任务可以自由实现奖励函数的逻辑。在这个例子中，我们实现了一个使用Pytorch "
"JIT功能来计算奖励函数的各个组成部分的函数。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:149
msgid "Defining Observations"
msgstr "定义观察"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:151
msgid ""
"The observation buffer should be computed in the ``_get_observations(self)``"
" function, which constructs the observation buffer for the environment. At "
"the end of this API, a dictionary should be returned that contains "
"``policy`` as the key, and the full observation buffer as the value. For "
"asymmetric policies, the dictionary should also include the key ``critic`` "
"and the states buffer as the value."
msgstr ""
"观察缓冲区应该在 ``_get_observations(self)`` "
"函数中计算，这个函数构造了环境的观察缓冲区。在这个API的末尾，应该返回一个包含 ``policy`` "
"作为key和完整观察缓冲区作为value的字典。对于异态策略，字典还应该包括 key ``critic`` 和状态缓冲区作为value。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:162
msgid "Computing Dones and Performing Resets"
msgstr "计算dones和执行resets"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:164
msgid ""
"Populating the ``dones`` buffer should be done in the ``_get_dones(self)`` "
"method. This method is free to implement logic that computes which "
"environments would need to be reset and which environments have reached the "
"episode length limit. Both results should be returned by the "
"``_get_dones(self)`` function, in the form of a tuple of boolean tensors."
msgstr ""
"``dones`` 缓冲区的填充应该在 ``_get_dones(self)`` "
"方法中完成。这个方法可以自由地实现逻辑，来计算哪些环境需要被reset，哪些环境已经达到了episode长度的限制。两个结果都应该由 "
"``_get_dones(self)`` 函数返回，以布尔张量的形式返回。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:173
msgid ""
"Once the indices for environments requiring reset have been computed, the "
"``_reset_idx(self, env_ids)`` function performs the reset operations on "
"those environments. Within this function, new states for the environments "
"requiring reset should be set directly into simulation."
msgstr ""
"一旦计算出需要reset的环境的索引，``_reset_idx(self, env_ids)`` "
"函数就执行这些环境的reset操作。在这个函数内，对需要reset的环境的新状态应该被直接设置到模拟中。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:182
msgid "Applying Actions"
msgstr "应用actions"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:184
msgid ""
"There are two APIs that are designed for working with actions. The "
"``_pre_physics_step(self, actions)`` takes in actions from the policy as an "
"argument and is called once per RL step, prior to taking any physics steps. "
"This function can be used to process the actions buffer from the policy and "
"cache the data in a class variable for the environment."
msgstr ""
"有两个API被设计用来处理actions。 ``_pre_physics_step(self, actions)`` "
"以来自策略的actions作为参数，并且在每个RL步骤之前调用，先于任何物理步骤。这个函数可以用来处理来自policy的actions缓冲区，并且在环境的类变量中缓存数据。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:192
msgid ""
"The ``_apply_action(self)`` API is called ``decimation`` number of times for"
" each RL step, prior to taking each physics step. This provides more "
"flexibility for environments where actions should be applied for each "
"physics step."
msgstr ""
"``_apply_action(self)`` API被称为每个RL步骤 ``decimation`` "
"次，先于每个物理步骤。这为那些需要对每个物理步骤应用actions的环境提供了更大的灵活性。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:202
msgid "The Code Execution"
msgstr "代码执行"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:204
msgid ""
"To run training for the direct workflow Cartpole environment, we can use the"
" following command:"
msgstr "为了对直接工作流cartpole环境进行训练，我们可以使用以下命令："

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:210
msgid ""
"All direct workflow tasks have the suffix ``-Direct`` added to the task name"
" to differentiate the implementation style."
msgstr "所有直接工作流任务的后缀都增加 ``-Direct`` 以区分实现的风格。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:214
msgid "Domain Randomization"
msgstr "领域随机化"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:216
msgid ""
"In the direct workflow, domain randomization configuration uses the "
":class:`~omni.isaac.lab.utils.configclass` module to specify a configuration"
" class consisting of :class:`~managers.EventTermCfg` variables."
msgstr ""
"在直接工作流中，领域随机化配置使用 :class:`~omni.isaac.lab.utils.configclass` 模块来指定由 "
":class:`~managers.EventTermCfg` 变量组成的配置 class。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:219
msgid "Below is an example of a configuration class for domain randomization:"
msgstr "下面是领域随机化的一个配置类的示例"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:259
msgid ""
"Each ``EventTerm`` object is of the :class:`~managers.EventTermCfg` class "
"and takes in a ``func`` parameter for specifying the function to call during"
" randomization, a ``mode`` parameter, which can be ``startup``, ``reset`` or"
" ``interval``. THe ``params`` dictionary should provide the necessary "
"arguments to the function that is specified in the ``func`` parameter. "
"Functions specified as ``func`` for the ``EventTerm`` can be found in the "
":class:`~envs.mdp.events` module."
msgstr ""
"每个 ``EventTerm`` 对象都是 :class:`~managers.EventTermCfg` class，并接受 ``func`` "
"参数来指定随机化期间要调用的函数，``mode`` 参数，可以是 ``startup`` ，``reset`` 或 ``interval`` 。 "
"``params`` 字典应该提供函数指定 ``func`` 参数所需要的必要参数。作为 ``EventTerm`` 的 ``func`` "
"指定的函数可以在 :class:`~envs.mdp.events` 模块中找到。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:265
msgid ""
"Note that as part of the ``\"asset_cfg\": SceneEntityCfg(\"robot\", "
"body_names=\".*\")`` parameter, the name of the actor ``\"robot\"`` is "
"provided, along with the body or joint names specified as a regex "
"expression, which will be the actors and bodies/joints that will have "
"randomization applied."
msgstr ""
"注意，作为 ``\"asset_cfg\": SceneEntityCfg(\"robot\", body_names=\".*\")`` "
"参数的一部分，提供了actor ``\"robot\"`` "
"的名称，以及作为正则表达式指定的body或joint名称，这些名称将是将随机化应用于的actors和body/joints。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:269
msgid ""
"Once the ``configclass`` for the randomization terms have been set up, the "
"class must be added to the base config class for the task and be assigned to"
" the variable ``events``."
msgstr "一旦随机化术语的 ``configclass`` 被设置好，这个class必须被添加到任务的基类配置、并分配给变量 ``events`` 。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:280
msgid "Action and Observation Noise"
msgstr "Action和Observation噪声"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:282
msgid ""
"Actions and observation noise can also be added using the "
":class:`~utils.configclass` module. Action and observation noise configs "
"must be added to the main task config using the ``action_noise_model`` and "
"``observation_noise_model`` variables:"
msgstr ""
"可以使用 :class:`~utils.configclass` 模块来添加action和观察噪声配置。动作和观察噪声配置必须被添加到主任务配置，使用 "
"``action_noise_model`` and ``observation_noise_model`` 变量："

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:304
msgid ""
":class:`~.utils.noise.NoiseModelWithAdditiveBiasCfg` can be used to sample "
"both uncorrelated noise per step as well as correlated noise that is re-"
"sampled at reset time."
msgstr ""
":class:`~.utils.noise.NoiseModelWithAdditiveBiasCfg` "
"可以用来对每次步长进行不相关噪声的抽样，并且对在重设时进行重新抽样的相关噪声进行抽样。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:307
msgid ""
"The ``noise_cfg`` term specifies the Gaussian distribution that will be "
"sampled at each step for all environments. This noise will be added to the "
"corresponding actions and observations buffers at every step."
msgstr ""
"``noise_cfg`` "
"项为每个环境在每步中进行抽样的高斯分布进行了指定。这些噪声将被添加到每个步骤的对应的actions和observations缓冲区中。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:311
msgid ""
"The ``bias_noise_cfg`` term specifies the Gaussian distribution for the "
"correlated noise that will be sampled at reset time for the environments "
"being reset. The same noise will be applied each step for the remaining of "
"the episode for the environments and resampled at the next reset."
msgstr ""
"``bias_noise_cfg`` "
"项指定了在对被重置的环境进行重新抽样时，在重置时间进行抽样的相关噪声的高斯分布。相同的噪声将被应用于每个步骤，直到环境的剩余阶段，然后在下一次重置时重新抽样。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:316
msgid ""
"If only per-step noise is desired, :class:`~utils.noise.GaussianNoiseCfg` "
"can be used to specify an additive Gaussian distribution that adds the "
"sampled noise to the input buffer."
msgstr ""
"如果仅仅需要每部噪声，则可以使用 :class:`~utils.noise.GaussianNoiseCfg` "
"来指定一个对输入缓冲区添加抽样噪声的加性高斯分布噪声。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:328
msgid ""
"In this tutorial, we learnt how to create a direct workflow task environment"
" for reinforcement learning. We do this by extending the base environment to"
" include the scene setup, actions, dones, reset, reward and observaion "
"functions."
msgstr ""
"在本教程中，我们学习了如何为强化学习创建直接工作流任务环境。我们通过扩展基础环境来实现scene setup, actions, dones, "
"reset, reward和observaion函数。"

#: ../../source/tutorials/03_envs/create_direct_rl_env.rst:331
msgid ""
"While it is possible to manually create an instance of "
":class:`~omni.isaac.lab.envs.DirectRLEnv` class for a desired task, this is "
"not scalable as it requires specialized scripts for each task. Thus, we "
"exploit the :meth:`gymnasium.make` function to create the environment with "
"the gym interface. We will learn how to do this in the next tutorial."
msgstr ""
"虽然可以手动创建一个 :class:`~omni.isaac.lab.envs.DirectRLEnv` "
"类的实例以适用于所需的任务，但这样做是不可扩展的，因为这需要为每个任务编写专门的脚本。因此，我们利用 :meth:`gymnasium.make` "
"函数来使用gym接口创建环境。我们将在下一个教程中学习如何做到这一点。"
